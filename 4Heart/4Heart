import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
 

df = pd.read_csv("Heart.csv")

df.head()


# a) Data Cleaning
df = df.drop_duplicates()

df.describe()

df.info()

df.isna().sum()

# b) Data Integration
df.fbs.unique()

df1 = df[['age','cp','chol','thalachh']]

df2 = df[['exng','slp','output']]

merging=pd.concat([df1,df2],axis=1)
merging

# d) Error Correcting
df.columns

def remove_outliers(column):
    Q1 = column.quantile(0.25)
    Q3 = column.quantile(0.75)
    IQR = Q3 - Q1
    threshold = 1.5 * IQR
    outlier_mask = (column < Q1 - threshold) | (column > Q3 + threshold)
    return column[~outlier_mask]


col_name = ['cp','thalachh','exng','oldpeak','slp','caa']
for col in col_name:
    df[col] = remove_outliers(df[col])


df = df.dropna()

df = df.drop('fbs',axis=1)

x = df[['cp','thalachh','exng','oldpeak','slp','caa']]
y = df.output
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)
x_train.shape,x_test.shape,y_train.shape,y_test.shape

from sklearn.preprocessing import StandardScaler

# e) Data transformation
scaler = StandardScaler()

x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# e) Data model building
model = LogisticRegression()
model.fit(x_train_scaled, y_train)


# Make predictions on the test set
y_pred = model.predict(x_test_scaled)

# Evaluate the model's accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Logistic Regression Accuracy:", accuracy)
print("Classification Report:\n", classification_report(y_test, y_pred))


# Classification model using Decision Tree
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier(criterion='entropy')
dtc.fit(x_train_scaled,y_train)
y_pred_dtc=dtc.predict(x_test_scaled)


print("Decision Tree Accuracy:", accuracy_score(y_test, y_pred_dtc))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dtc))
print("Classification Report:\n", classification_report(y_test, y_pred_dtc))


#####theory 
import numpy as np â€” Imports NumPy for numerical operations.

import pandas as pd â€” Imports pandas for data manipulation and analysis.

from sklearn.model_selection import train_test_split â€” Used to split the dataset into training and testing sets.

from sklearn.preprocessing import StandardScaler â€” Used to standardize (scale) the features.

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report â€” Used for evaluating model performance.

from sklearn.linear_model import LogisticRegression â€” Imports logistic regression model.

from sklearn.tree import DecisionTreeClassifier â€” Imports decision tree classifier.

df = pd.read_csv("Heart.csv") â€” Reads the CSV file into a pandas DataFrame.

df.head() â€” Displays the first 5 rows of the DataFrame.





//theory
a) Data Cleaning

df.drop_duplicates() â€” Removes duplicate records from the data.

df.describe() â€” Shows statistical summary like mean, min, max, etc.

df.info() â€” Shows data types and non-null count of each column.

df.isna().sum() â€” Checks for missing (null) values in each column.

b) Data Integration

df.fbs.unique() â€” Shows all unique values in the 'fbsasting blood sugar' column.

df1 = df[['age','cp','chol','thalachh']] â€” Creates first subset of selected columns.

df2 = df[['exng','slp','output']] â€” Creates second subset.

merging = pd.concat([df1, df2], axis=1) â€” Merges the two subsets side by side.

d) Error Correcting

df.columns â€” Lists all column names in the dataset.

Defines a function remove_outliers(column) â€” Calculates IQR and removes data points beyond 1.5 times IQR.

A list of column names is defined: ['cp','thalachh','exng','oldpeak','slp','caa']

A loop applies the remove_outliers() function to each column in that list.

df.dropna() â€” Removes any rows that now have missing values due to outlier removal.

df.drop('fbs', axis=1) â€” Removes the 'fbs' column from the dataset.

Feature Selection and Splitting

x = df[['cp','thalachh','exng','oldpeak','slp','caa']] â€” Selects features for the model.

y = df.output â€” Sets the target variable (output).

train_test_split(x, y, test_size=0.2, random_state=0) â€” Splits data into 80% training and 20% testing.

e) Data Transformation

StandardScaler() is used â€” Scales features to have mean 0 and standard deviation 1.

fit_transform() is applied on training data, and transform() on test data.

Model Building â€” Logistic Regression

LogisticRegression() is initialized and trained using the scaled training data.

Predictions are made on test data.

Accuracy and classification report (precision, recall, F1-score) are printed.

Model Building â€” Decision Tree

DecisionTreeClassifier(criterion='entropy') is used â€” Builds a tree using information gain.

The model is trained and tested similarly.

Accuracy, confusion matrix, and classification report for the decision tree are printed.


Precision
Definition:

Precision tells how many of the predicted positives were actually correct.

Formula:

Precision
=
TrueÂ PositivesÂ (TP)
TrueÂ PositivesÂ (TP)
+
FalseÂ PositivesÂ (FP)
Precision= 
TrueÂ PositivesÂ (TP)+FalseÂ PositivesÂ (FP)
TrueÂ PositivesÂ (TP)
â€‹
 
ðŸ§  Example: If a model predicts 10 people have a disease, but only 6 really do â†’ Precision = 6/10 = 0.6

âœ… Recall (a.k.a. Sensitivity or True Positive Rate)
Definition:

Recall tells how many of the actual positives were correctly predicted.

Formula:

Recall
=
TrueÂ PositivesÂ (TP)
TrueÂ PositivesÂ (TP)
+
FalseÂ NegativesÂ (FN)
Recall= 
TrueÂ PositivesÂ (TP)+FalseÂ NegativesÂ (FN)
TrueÂ PositivesÂ (TP)
â€‹
 
ðŸ§  Example: Out of 8 real patients with a disease, if the model finds 6 â†’ Recall = 6/8 = 0.75

âœ… F1-Score
Definition:

F1-score is the harmonic mean of precision and recall; it balances both.

Formula:

F1Â Score
=
2
Ã—
Precision
Ã—
Recall
Precision
+
Recall
F1Â Score=2Ã— 
Precision+Recall
PrecisionÃ—Recall
â€‹
 
ðŸ§  Use it when you want a balance between Precision and Recall (e.g., fraud detection, medical diagnoses).

âœ… Support
Definition:

Support is the number of actual instances of each class in the dataset.

Would you like a table showing how these metrics are calculated from a confusion matrix?







You said:
what is decistion tree classifier 


ChatGPT said:
A Decision Tree Classifier is a machine learning algorithm used for classifying data into categories or labels by learning decision rules from the data.

âœ… Simple Definition:
A Decision Tree Classifier splits the data into smaller and smaller parts using if-else conditions, just like how a flowchart or real-life decisions work.

ðŸ§  Real-Life Example:
Imagine you're trying to decide what to wear:

Is it raining?

Yes â†’ Take an umbrella.

No â†’ Is it cold?

Yes â†’ Wear a jacket.

No â†’ T-shirt is fine.

This is a decision tree!

âœ… In Machine Learning:
Each internal node tests a condition (like age > 50),
Each branch is the outcome (yes/no),
Each leaf node is a final class label (like "has heart disease" or "no heart disease").
ðŸ“š Viva Questions and Answers
Q1. What is the goal of this project?
A. To predict whether a person has heart disease using machine learning models.

Q2. Why did you remove duplicates?
A. To prevent repeated data from biasing the model and to improve training quality.

Q3. What does StandardScaler do?
A. It standardizes features to have a mean of 0 and standard deviation of 1.

Q4. Why did you remove outliers?
A. Outliers can mislead model training and reduce accuracy.

Q5. What is the significance of dropping the 'fbs' column?
A. The 'fbs' column might have been uninformative or had low variance.

Q6. Why do we split data into train and test sets?
A. To evaluate how well the model generalizes to unseen data.

Q7. Why use Logistic Regression?
A. It is simple, fast, and effective for binary classification tasks.

Q8. What is the role of Decision Tree?
A. It is a non-linear classifier that splits data based on conditions to predict outcomes.

Q9. What is entropy in Decision Trees?
A. Entropy measures impurity; lower entropy means more pure nodes.

Q10. What is the confusion matrix used for?
A. It shows the counts of true positives, false positives, true negatives, and false negatives.

Q11. What does the classification report include?
A. Precision, recall, F1-score, and support for each class.

Q12. What performance metrics did you use?
A. Accuracy, confusion matrix, precision, recall, and F1-score.


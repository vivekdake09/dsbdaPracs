Step 1:
Project create practical3 > package create with prac3 >  class  create Process 

Step 2:
code copy paste :
package prac3;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class Process {

  public static class IPMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text ip = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      // Assuming the IP address is the first token in each line
      StringTokenizer itr = new StringTokenizer(value.toString());
      if (itr.hasMoreTokens()) {
        ip.set(itr.nextToken());
        context.write(ip, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "IP address count");
    job.setJarByClass(Process.class);
    job.setMapperClass(IPMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }


Step 3:
2 jar files import  : 
(1) right click project > build path > add external archives > file system > usr/lib/hadoop -> hadoop-common-2.6.0-cdh5.13.0.jar

(2) right click project > build path > add external archives > file system > usr/lib/hadoop-0.20-mapreduce  -> hadoop-core-2.6.0-mr1-cdh5.13.0.jar

Step 4:
Copy logfile  from url : https://mapreduceprogram.blogspot.com

Step 5:
create document empty on desktop 

Step 6:
open terminal and follow following commands:
(1) pwd

(2) cd ../

(3) pwd

(4) hadoop fs -put Desktop/log.txt log.txt

(5)  hadoop jar Process.jar prac3.Process log.txt dir16

(6) hadoop fs -cat dir16/part-r-00000





//theory
Purpose of the Code (in simple words)
This code is written using MapReduce (a method used in Hadoop, a system for handling big data). The purpose of this code is:

To read a system log file and count how many times each user (or IP address) has appeared (logged in). It shows which IP address used the system most.

This is a first step in solving the goal:

"List out users who logged in for the maximum period."
We first count how often each user/IP appeared, and from that we can assume who's logged in the most.

ğŸ“„ Full Code Explanation (Line by Line, in kid-friendly language)
java
Copy
Edit
package prac3;
ğŸ‘‰ This line says: "Iâ€™m putting this code in a folder/package called prac3."

java
Copy
Edit
import java.io.IOException;
import java.util.StringTokenizer;
ğŸ‘‰ We're using some tools:

IOException handles errors when reading files.

StringTokenizer helps break a sentence into words.

java
Copy
Edit
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
ğŸ‘‰ These are Hadoop tools we use to:

Create jobs,

Read input files, and

Write output files.

ğŸ”· Mapper Class
java
Copy
Edit
public static class IPMapper extends Mapper<Object, Text, Text, IntWritable>{
ğŸ‘‰ We are creating a part of the program called IPMapper (a Mapper). It takes each line of the log file and does something with it.

java
Copy
Edit
    private final static IntWritable one = new IntWritable(1);
    private Text ip = new Text();
ğŸ‘‰ These lines set up:

one â†’ means â€œcount this as 1 time.â€

ip â†’ we will store the IP address in it.

java
Copy
Edit
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
ğŸ‘‰ This function will run for every line in the file.

java
Copy
Edit
      StringTokenizer itr = new StringTokenizer(value.toString());
ğŸ‘‰ It breaks the line into words (tokens).

java
Copy
Edit
      if (itr.hasMoreTokens()) {
        ip.set(itr.nextToken());
        context.write(ip, one);
      }
ğŸ‘‰ If the line has something:

Take the first word (assumed to be an IP),

Save it in ip,

Tell Hadoop: â€œI found this IP, count it as 1.â€

ğŸ”¶ Reducer Class
java
Copy
Edit
public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
ğŸ‘‰ Now we create the Reducer, which will add up all the 1s for each IP.

java
Copy
Edit
    private IntWritable result = new IntWritable();
ğŸ‘‰ We create a place to store the final number.

java
Copy
Edit
    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
ğŸ‘‰ This function gets called for each IP address.

java
Copy
Edit
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
ğŸ‘‰ Add up all the 1s for this IP.

java
Copy
Edit
      result.set(sum);
      context.write(key, result);
ğŸ‘‰ Save and write the result like:
192.168.1.1 â†’ 5

ğŸ”¸ Main Method
java
Copy
Edit
public static void main(String[] args) throws Exception {
ğŸ‘‰ This is where the program starts running.

java
Copy
Edit
    Configuration conf = new Configuration();
ğŸ‘‰ Set up the Hadoop configuration.

java
Copy
Edit
    Job job = Job.getInstance(conf, "IP address count");
ğŸ‘‰ Create a job called â€œIP address countâ€.

java
Copy
Edit
    job.setJarByClass(Process.class);
    job.setMapperClass(IPMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
ğŸ‘‰ Tell Hadoop which classes to use:

Mapper

Combiner (does quick addition before Reduce)

Reducer

java
Copy
Edit
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
ğŸ‘‰ Say what kind of data we are outputting:

Key â†’ text (IP address)

Value â†’ integer (count)

java
Copy
Edit
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
ğŸ‘‰ Get the input and output file paths from user input.

java
Copy
Edit
    System.exit(job.waitForCompletion(true) ? 0 : 1);
ğŸ‘‰ Start the job and exit when done.

âœ… Sample Output
Copy
Edit
192.168.0.1	5
192.168.0.2	3
10.0.0.1	7
ğŸ‘‰ This means:

IP 10.0.0.1 appeared 7 times (used most).

ğŸ§  Viva Questions & Answers (Simple)
Q1: What is the purpose of this MapReduce program?
A: It counts how many times each IP address appears in the log file.

Q2: What does the Mapper do?
A: It reads each line, takes the first word (IP address), and gives it a value of 1.

Q3: What does the Reducer do?
A: It adds all the 1s for each IP and gives the total number of times that IP appeared.

Q4: What is the role of Combiner here?
A: It helps to reduce the work of the Reducer by adding values early (optional mini-reducer).

Q5: What data types are used for key and value?
A: Key is Text (IP address) and Value is IntWritable (number).

Q6: How is this useful in log file analysis?
A: It helps us find out which user (IP) used the system the most.

Q7: What is Hadoop used for?
A: Hadoop is used for processing and storing very large data using many computers.

Q8: Can this code run on one computer?
A: Yes, in pseudo-distributed mode (one computer behaves like a mini-Hadoop system).
Question	Answer
What is MapReduce?	A programming model for processing large data sets with a distributed algorithm on a cluster.
What is the role of a Mapper?	It processes input data line by line and outputs intermediate key-value pairs.
What does the Reducer do?	It aggregates values for the same key and produces the final output.
What is the purpose of the Combiner?	It's an optimization that performs a local reduce before sending data to the actual Reducer.
What does Text and IntWritable mean?	These are Hadoop's writable versions of Java's String and Integer types.
How is the log file processed?	The Mapper extracts IPs from each log line, and the Reducer counts their occurrences.
Why is StringTokenizer used?	To split each line into tokens to extract the IP address.
What is the pseudo-distributed mode?	A Hadoop configuration where all daemons run on a single machine but simulate a real cluster.
Why do we use context.write()?	To emit key-value pairs from the Mapper or Reducer.
How can you find the IP with the longest login duration?	Count IP occurrences (as in this code) or use timestamp-based duration logic in a more advanced setup.
